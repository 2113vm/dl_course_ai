{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==0.4.1 scikit-learn==0.20.2 bokeh==0.13.0 gensim==3.6.0 nltk\n",
    "# !pip install -U torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/vitaly/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/vitaly/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'NOUN', 'NUM', 'X', 'PRON', 'ADP', 'DET', 'CONJ', 'ADV', 'PRT', 'ADJ', 'VERB', '.'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._tagset_size = tagset_size\n",
    "        self._word_emb_dim = word_emb_dim\n",
    "        self._lstm_hidden_dim = lstm_hidden_dim\n",
    "        self._lstm_layer_count = lstm_layers_count\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45441, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2ind), len(tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "# seq_len, batch_size, target_size\n",
    "# для каждого элемента из последовательности длинной seq_len есть вектор размера target_size. \n",
    "# таких последовательностей в количестве batch_size\n",
    "\n",
    "mask = y_batch != 0\n",
    "logit_mask = logits.argmax(dim=2) * mask\n",
    "y_batch_mask = y_batch * mask\n",
    "cur_sum_count = mask.sum()\n",
    "cur_correct_count = ((logit_mask == y_batch_mask).sum() - (~mask).sum()).float()\n",
    "\n",
    "acc = cur_correct_count / cur_sum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0652)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.permute(0, 2, 1), y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5773, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.permute(0, 2, 1), y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                mask = y_batch != 0\n",
    "                logit_mask = logits.argmax(dim=2) * mask\n",
    "                y_batch_mask = y_batch * mask\n",
    "                cur_sum_count = mask.sum()\n",
    "                cur_correct_count = ((logit_mask == y_batch_mask).sum() - (~mask).sum()).float()\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.47134, Accuracy = 68.77%: 100%|██████████| 572/572 [00:03<00:00, 157.10it/s]\n",
      "[1 / 50]   Val: Loss = 0.16873, Accuracy = 81.36%: 100%|██████████| 13/13 [00:00<00:00, 125.36it/s]\n",
      "[2 / 50] Train: Loss = 0.16515, Accuracy = 85.15%: 100%|██████████| 572/572 [00:03<00:00, 159.27it/s]\n",
      "[2 / 50]   Val: Loss = 0.13441, Accuracy = 86.81%: 100%|██████████| 13/13 [00:00<00:00, 122.14it/s]\n",
      "[3 / 50] Train: Loss = 0.12621, Accuracy = 88.52%: 100%|██████████| 572/572 [00:03<00:00, 163.31it/s]\n",
      "[3 / 50]   Val: Loss = 0.12924, Accuracy = 88.91%: 100%|██████████| 13/13 [00:00<00:00, 118.63it/s]\n",
      "[4 / 50] Train: Loss = 0.10654, Accuracy = 90.04%: 100%|██████████| 572/572 [00:03<00:00, 158.58it/s]\n",
      "[4 / 50]   Val: Loss = 0.13681, Accuracy = 89.97%: 100%|██████████| 13/13 [00:00<00:00, 124.87it/s]\n",
      "[5 / 50] Train: Loss = 0.09485, Accuracy = 91.06%: 100%|██████████| 572/572 [00:03<00:00, 157.58it/s]\n",
      "[5 / 50]   Val: Loss = 0.13930, Accuracy = 90.66%: 100%|██████████| 13/13 [00:00<00:00, 123.06it/s]\n",
      "[6 / 50] Train: Loss = 0.08635, Accuracy = 91.71%: 100%|██████████| 572/572 [00:03<00:00, 157.12it/s]\n",
      "[6 / 50]   Val: Loss = 0.14017, Accuracy = 91.05%: 100%|██████████| 13/13 [00:00<00:00, 130.97it/s]\n",
      "[7 / 50] Train: Loss = 0.08010, Accuracy = 92.22%: 100%|██████████| 572/572 [00:03<00:00, 160.25it/s]\n",
      "[7 / 50]   Val: Loss = 0.14662, Accuracy = 91.38%: 100%|██████████| 13/13 [00:00<00:00, 126.98it/s]\n",
      "[8 / 50] Train: Loss = 0.07458, Accuracy = 92.64%: 100%|██████████| 572/572 [00:03<00:00, 160.74it/s]\n",
      "[8 / 50]   Val: Loss = 0.15106, Accuracy = 91.55%: 100%|██████████| 13/13 [00:00<00:00, 121.85it/s]\n",
      "[9 / 50] Train: Loss = 0.07020, Accuracy = 92.91%: 100%|██████████| 572/572 [00:03<00:00, 155.42it/s]\n",
      "[9 / 50]   Val: Loss = 0.17422, Accuracy = 91.94%: 100%|██████████| 13/13 [00:00<00:00, 131.14it/s]\n",
      "[10 / 50] Train: Loss = 0.06696, Accuracy = 93.18%: 100%|██████████| 572/572 [00:03<00:00, 159.46it/s]\n",
      "[10 / 50]   Val: Loss = 0.18469, Accuracy = 92.04%: 100%|██████████| 13/13 [00:00<00:00, 124.68it/s]\n",
      "[11 / 50] Train: Loss = 0.06371, Accuracy = 93.36%: 100%|██████████| 572/572 [00:03<00:00, 159.94it/s]\n",
      "[11 / 50]   Val: Loss = 0.16840, Accuracy = 91.89%: 100%|██████████| 13/13 [00:00<00:00, 118.83it/s]\n",
      "[12 / 50] Train: Loss = 0.06162, Accuracy = 93.52%: 100%|██████████| 572/572 [00:03<00:00, 157.30it/s]\n",
      "[12 / 50]   Val: Loss = 0.19572, Accuracy = 92.09%: 100%|██████████| 13/13 [00:00<00:00, 125.11it/s]\n",
      "[13 / 50] Train: Loss = 0.05921, Accuracy = 93.66%: 100%|██████████| 572/572 [00:03<00:00, 158.19it/s]\n",
      "[13 / 50]   Val: Loss = 0.18799, Accuracy = 92.05%: 100%|██████████| 13/13 [00:00<00:00, 123.56it/s]\n",
      "[14 / 50] Train: Loss = 0.05753, Accuracy = 93.74%: 100%|██████████| 572/572 [00:03<00:00, 153.01it/s]\n",
      "[14 / 50]   Val: Loss = 0.19311, Accuracy = 92.11%: 100%|██████████| 13/13 [00:00<00:00, 119.72it/s]\n",
      "[15 / 50] Train: Loss = 0.05579, Accuracy = 93.86%: 100%|██████████| 572/572 [00:03<00:00, 153.89it/s]\n",
      "[15 / 50]   Val: Loss = 0.22380, Accuracy = 92.17%: 100%|██████████| 13/13 [00:00<00:00, 126.29it/s]\n",
      "[16 / 50] Train: Loss = 0.05464, Accuracy = 93.89%: 100%|██████████| 572/572 [00:03<00:00, 157.83it/s]\n",
      "[16 / 50]   Val: Loss = 0.21278, Accuracy = 92.26%: 100%|██████████| 13/13 [00:00<00:00, 125.52it/s]\n",
      "[17 / 50] Train: Loss = 0.05355, Accuracy = 94.01%: 100%|██████████| 572/572 [00:03<00:00, 157.95it/s]\n",
      "[17 / 50]   Val: Loss = 0.22552, Accuracy = 92.30%: 100%|██████████| 13/13 [00:00<00:00, 122.57it/s]\n",
      "[18 / 50] Train: Loss = 0.05233, Accuracy = 94.03%: 100%|██████████| 572/572 [00:03<00:00, 158.12it/s]\n",
      "[18 / 50]   Val: Loss = 0.25952, Accuracy = 92.34%: 100%|██████████| 13/13 [00:00<00:00, 112.71it/s]\n",
      "[19 / 50] Train: Loss = 0.05132, Accuracy = 94.13%: 100%|██████████| 572/572 [00:03<00:00, 157.99it/s]\n",
      "[19 / 50]   Val: Loss = 0.24470, Accuracy = 92.30%: 100%|██████████| 13/13 [00:00<00:00, 122.13it/s]\n",
      "[20 / 50] Train: Loss = 0.05033, Accuracy = 94.23%: 100%|██████████| 572/572 [00:03<00:00, 155.38it/s]\n",
      "[20 / 50]   Val: Loss = 0.25955, Accuracy = 92.32%: 100%|██████████| 13/13 [00:00<00:00, 128.90it/s]\n",
      "[21 / 50] Train: Loss = 0.04958, Accuracy = 94.27%: 100%|██████████| 572/572 [00:03<00:00, 156.60it/s]\n",
      "[21 / 50]   Val: Loss = 0.25674, Accuracy = 92.27%: 100%|██████████| 13/13 [00:00<00:00, 129.33it/s]\n",
      "[22 / 50] Train: Loss = 0.04904, Accuracy = 94.31%: 100%|██████████| 572/572 [00:03<00:00, 158.26it/s]\n",
      "[22 / 50]   Val: Loss = 0.28234, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 128.67it/s]\n",
      "[23 / 50] Train: Loss = 0.04815, Accuracy = 94.36%: 100%|██████████| 572/572 [00:03<00:00, 157.82it/s]\n",
      "[23 / 50]   Val: Loss = 0.26234, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 120.94it/s]\n",
      "[24 / 50] Train: Loss = 0.04729, Accuracy = 94.41%: 100%|██████████| 572/572 [00:03<00:00, 157.23it/s]\n",
      "[24 / 50]   Val: Loss = 0.29790, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 131.44it/s]\n",
      "[25 / 50] Train: Loss = 0.04712, Accuracy = 94.47%: 100%|██████████| 572/572 [00:03<00:00, 153.61it/s]\n",
      "[25 / 50]   Val: Loss = 0.27665, Accuracy = 92.28%: 100%|██████████| 13/13 [00:00<00:00, 126.82it/s]\n",
      "[26 / 50] Train: Loss = 0.04626, Accuracy = 94.52%: 100%|██████████| 572/572 [00:03<00:00, 157.77it/s]\n",
      "[26 / 50]   Val: Loss = 0.29261, Accuracy = 92.23%: 100%|██████████| 13/13 [00:00<00:00, 109.78it/s]\n",
      "[27 / 50] Train: Loss = 0.04583, Accuracy = 94.58%: 100%|██████████| 572/572 [00:03<00:00, 157.45it/s]\n",
      "[27 / 50]   Val: Loss = 0.30505, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 119.98it/s]\n",
      "[28 / 50] Train: Loss = 0.04553, Accuracy = 94.57%: 100%|██████████| 572/572 [00:03<00:00, 152.63it/s]\n",
      "[28 / 50]   Val: Loss = 0.29303, Accuracy = 92.23%: 100%|██████████| 13/13 [00:00<00:00, 123.99it/s]\n",
      "[29 / 50] Train: Loss = 0.04557, Accuracy = 94.59%: 100%|██████████| 572/572 [00:03<00:00, 156.48it/s]\n",
      "[29 / 50]   Val: Loss = 0.30126, Accuracy = 92.32%: 100%|██████████| 13/13 [00:00<00:00, 125.00it/s]\n",
      "[30 / 50] Train: Loss = 0.04487, Accuracy = 94.66%: 100%|██████████| 572/572 [00:03<00:00, 158.07it/s]\n",
      "[30 / 50]   Val: Loss = 0.30515, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 127.48it/s]\n",
      "[31 / 50] Train: Loss = 0.04451, Accuracy = 94.66%: 100%|██████████| 572/572 [00:03<00:00, 158.22it/s]\n",
      "[31 / 50]   Val: Loss = 0.31148, Accuracy = 92.40%: 100%|██████████| 13/13 [00:00<00:00, 118.95it/s]\n",
      "[32 / 50] Train: Loss = 0.04422, Accuracy = 94.71%: 100%|██████████| 572/572 [00:03<00:00, 157.15it/s]\n",
      "[32 / 50]   Val: Loss = 0.30277, Accuracy = 92.35%: 100%|██████████| 13/13 [00:00<00:00, 123.99it/s]\n",
      "[33 / 50] Train: Loss = 0.04389, Accuracy = 94.74%: 100%|██████████| 572/572 [00:03<00:00, 158.44it/s]\n",
      "[33 / 50]   Val: Loss = 0.32169, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 126.03it/s]\n",
      "[34 / 50] Train: Loss = 0.04361, Accuracy = 94.73%: 100%|██████████| 572/572 [00:03<00:00, 157.92it/s]\n",
      "[34 / 50]   Val: Loss = 0.33964, Accuracy = 92.32%: 100%|██████████| 13/13 [00:00<00:00, 113.23it/s]\n",
      "[35 / 50] Train: Loss = 0.04320, Accuracy = 94.77%: 100%|██████████| 572/572 [00:03<00:00, 157.89it/s]\n",
      "[35 / 50]   Val: Loss = 0.31227, Accuracy = 92.31%: 100%|██████████| 13/13 [00:00<00:00, 123.35it/s]\n",
      "[36 / 50] Train: Loss = 0.04302, Accuracy = 94.80%: 100%|██████████| 572/572 [00:03<00:00, 156.63it/s]\n",
      "[36 / 50]   Val: Loss = 0.35493, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 125.32it/s]\n",
      "[37 / 50] Train: Loss = 0.04297, Accuracy = 94.85%: 100%|██████████| 572/572 [00:03<00:00, 157.48it/s]\n",
      "[37 / 50]   Val: Loss = 0.34478, Accuracy = 92.49%: 100%|██████████| 13/13 [00:00<00:00, 129.58it/s]\n",
      "[38 / 50] Train: Loss = 0.04276, Accuracy = 94.88%: 100%|██████████| 572/572 [00:03<00:00, 157.27it/s]\n",
      "[38 / 50]   Val: Loss = 0.31181, Accuracy = 92.34%: 100%|██████████| 13/13 [00:00<00:00, 124.86it/s]\n",
      "[39 / 50] Train: Loss = 0.04259, Accuracy = 94.85%: 100%|██████████| 572/572 [00:03<00:00, 156.71it/s]\n",
      "[39 / 50]   Val: Loss = 0.37646, Accuracy = 92.45%: 100%|██████████| 13/13 [00:00<00:00, 127.20it/s]\n",
      "[40 / 50] Train: Loss = 0.04237, Accuracy = 94.86%: 100%|██████████| 572/572 [00:03<00:00, 158.09it/s]\n",
      "[40 / 50]   Val: Loss = 0.36653, Accuracy = 92.40%: 100%|██████████| 13/13 [00:00<00:00, 127.08it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.04229, Accuracy = 94.86%: 100%|██████████| 572/572 [00:03<00:00, 155.14it/s]\n",
      "[41 / 50]   Val: Loss = 0.35573, Accuracy = 92.39%: 100%|██████████| 13/13 [00:00<00:00, 128.15it/s]\n",
      "[42 / 50] Train: Loss = 0.04199, Accuracy = 94.92%: 100%|██████████| 572/572 [00:03<00:00, 157.48it/s]\n",
      "[42 / 50]   Val: Loss = 0.39202, Accuracy = 92.56%: 100%|██████████| 13/13 [00:00<00:00, 131.89it/s]\n",
      "[43 / 50] Train: Loss = 0.04163, Accuracy = 94.93%: 100%|██████████| 572/572 [00:03<00:00, 157.31it/s]\n",
      "[43 / 50]   Val: Loss = 0.36870, Accuracy = 92.65%: 100%|██████████| 13/13 [00:00<00:00, 126.87it/s]\n",
      "[44 / 50] Train: Loss = 0.04145, Accuracy = 94.94%: 100%|██████████| 572/572 [00:03<00:00, 155.41it/s]\n",
      "[44 / 50]   Val: Loss = 0.37259, Accuracy = 92.54%: 100%|██████████| 13/13 [00:00<00:00, 126.72it/s]\n",
      "[45 / 50] Train: Loss = 0.04127, Accuracy = 94.94%: 100%|██████████| 572/572 [00:03<00:00, 158.08it/s]\n",
      "[45 / 50]   Val: Loss = 0.37159, Accuracy = 92.46%: 100%|██████████| 13/13 [00:00<00:00, 127.92it/s]\n",
      "[46 / 50] Train: Loss = 0.04136, Accuracy = 94.92%: 100%|██████████| 572/572 [00:03<00:00, 155.56it/s]\n",
      "[46 / 50]   Val: Loss = 0.36969, Accuracy = 92.47%: 100%|██████████| 13/13 [00:00<00:00, 129.19it/s]\n",
      "[47 / 50] Train: Loss = 0.04109, Accuracy = 94.96%: 100%|██████████| 572/572 [00:03<00:00, 158.41it/s]\n",
      "[47 / 50]   Val: Loss = 0.36781, Accuracy = 92.48%: 100%|██████████| 13/13 [00:00<00:00, 126.16it/s]\n",
      "[48 / 50] Train: Loss = 0.04113, Accuracy = 94.94%: 100%|██████████| 572/572 [00:03<00:00, 156.93it/s]\n",
      "[48 / 50]   Val: Loss = 0.39914, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 125.89it/s]\n",
      "[49 / 50] Train: Loss = 0.04080, Accuracy = 94.95%: 100%|██████████| 572/572 [00:03<00:00, 157.21it/s]\n",
      "[49 / 50]   Val: Loss = 0.38449, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 130.62it/s]\n",
      "[50 / 50] Train: Loss = 0.04093, Accuracy = 94.97%: 100%|██████████| 572/572 [00:03<00:00, 158.00it/s]\n",
      "[50 / 50]   Val: Loss = 0.39090, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 126.42it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._tagset_size = tagset_size\n",
    "        self._word_emb_dim = word_emb_dim\n",
    "        self._lstm_hidden_dim = lstm_hidden_dim\n",
    "        self._lstm_layer_count = lstm_layers_count\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.35542, Accuracy = 72.95%: 100%|██████████| 572/572 [00:04<00:00, 116.12it/s]\n",
      "[1 / 50]   Val: Loss = 0.10025, Accuracy = 86.73%: 100%|██████████| 13/13 [00:00<00:00, 91.80it/s]\n",
      "[2 / 50] Train: Loss = 0.12582, Accuracy = 87.31%: 100%|██████████| 572/572 [00:04<00:00, 115.51it/s]\n",
      "[2 / 50]   Val: Loss = 0.06139, Accuracy = 91.60%: 100%|██████████| 13/13 [00:00<00:00, 86.96it/s]\n",
      "[3 / 50] Train: Loss = 0.09423, Accuracy = 90.36%: 100%|██████████| 572/572 [00:04<00:00, 116.90it/s]\n",
      "[3 / 50]   Val: Loss = 0.05132, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 93.69it/s]\n",
      "[4 / 50] Train: Loss = 0.07804, Accuracy = 91.78%: 100%|██████████| 572/572 [00:04<00:00, 115.79it/s]\n",
      "[4 / 50]   Val: Loss = 0.04369, Accuracy = 94.18%: 100%|██████████| 13/13 [00:00<00:00, 93.03it/s]\n",
      "[5 / 50] Train: Loss = 0.06864, Accuracy = 92.67%: 100%|██████████| 572/572 [00:04<00:00, 115.90it/s]\n",
      "[5 / 50]   Val: Loss = 0.04401, Accuracy = 94.69%: 100%|██████████| 13/13 [00:00<00:00, 92.90it/s]\n",
      "[6 / 50] Train: Loss = 0.06159, Accuracy = 93.27%: 100%|██████████| 572/572 [00:05<00:00, 114.12it/s]\n",
      "[6 / 50]   Val: Loss = 0.04198, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 87.99it/s]\n",
      "[7 / 50] Train: Loss = 0.05668, Accuracy = 93.74%: 100%|██████████| 572/572 [00:04<00:00, 116.24it/s]\n",
      "[7 / 50]   Val: Loss = 0.04362, Accuracy = 94.77%: 100%|██████████| 13/13 [00:00<00:00, 96.35it/s] \n",
      "[8 / 50] Train: Loss = 0.05217, Accuracy = 94.01%: 100%|██████████| 572/572 [00:04<00:00, 116.31it/s]\n",
      "[8 / 50]   Val: Loss = 0.04681, Accuracy = 94.54%: 100%|██████████| 13/13 [00:00<00:00, 98.04it/s]\n",
      "[9 / 50] Train: Loss = 0.04933, Accuracy = 94.24%: 100%|██████████| 572/572 [00:04<00:00, 116.69it/s]\n",
      "[9 / 50]   Val: Loss = 0.04117, Accuracy = 94.77%: 100%|██████████| 13/13 [00:00<00:00, 92.84it/s]\n",
      "[10 / 50] Train: Loss = 0.04640, Accuracy = 94.37%: 100%|██████████| 572/572 [00:04<00:00, 116.06it/s]\n",
      "[10 / 50]   Val: Loss = 0.04393, Accuracy = 94.48%: 100%|██████████| 13/13 [00:00<00:00, 92.52it/s]\n",
      "[11 / 50] Train: Loss = 0.04421, Accuracy = 94.50%: 100%|██████████| 572/572 [00:04<00:00, 117.86it/s]\n",
      "[11 / 50]   Val: Loss = 0.04809, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 90.97it/s]\n",
      "[12 / 50] Train: Loss = 0.04191, Accuracy = 94.73%: 100%|██████████| 572/572 [00:04<00:00, 117.20it/s]\n",
      "[12 / 50]   Val: Loss = 0.05117, Accuracy = 94.60%: 100%|██████████| 13/13 [00:00<00:00, 95.56it/s]\n",
      "[13 / 50] Train: Loss = 0.04057, Accuracy = 94.90%: 100%|██████████| 572/572 [00:04<00:00, 117.87it/s]\n",
      "[13 / 50]   Val: Loss = 0.05847, Accuracy = 94.61%: 100%|██████████| 13/13 [00:00<00:00, 90.65it/s]\n",
      "[14 / 50] Train: Loss = 0.03943, Accuracy = 94.92%: 100%|██████████| 572/572 [00:04<00:00, 116.11it/s]\n",
      "[14 / 50]   Val: Loss = 0.05634, Accuracy = 94.42%: 100%|██████████| 13/13 [00:00<00:00, 93.93it/s]\n",
      "[15 / 50] Train: Loss = 0.03817, Accuracy = 95.01%: 100%|██████████| 572/572 [00:04<00:00, 116.28it/s]\n",
      "[15 / 50]   Val: Loss = 0.06529, Accuracy = 94.26%: 100%|██████████| 13/13 [00:00<00:00, 90.77it/s]\n",
      "[16 / 50] Train: Loss = 0.03742, Accuracy = 95.03%: 100%|██████████| 572/572 [00:04<00:00, 115.34it/s]\n",
      "[16 / 50]   Val: Loss = 0.06329, Accuracy = 94.53%: 100%|██████████| 13/13 [00:00<00:00, 94.32it/s]\n",
      "[17 / 50] Train: Loss = 0.03664, Accuracy = 94.97%: 100%|██████████| 572/572 [00:04<00:00, 114.95it/s]\n",
      "[17 / 50]   Val: Loss = 0.06460, Accuracy = 94.79%: 100%|██████████| 13/13 [00:00<00:00, 90.87it/s]\n",
      "[18 / 50] Train: Loss = 0.03591, Accuracy = 95.05%: 100%|██████████| 572/572 [00:04<00:00, 115.66it/s]\n",
      "[18 / 50]   Val: Loss = 0.06564, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 94.97it/s] \n",
      "[19 / 50] Train: Loss = 0.03547, Accuracy = 95.04%: 100%|██████████| 572/572 [00:04<00:00, 115.13it/s]\n",
      "[19 / 50]   Val: Loss = 0.07103, Accuracy = 94.66%: 100%|██████████| 13/13 [00:00<00:00, 97.61it/s]\n",
      "[20 / 50] Train: Loss = 0.03496, Accuracy = 95.04%: 100%|██████████| 572/572 [00:04<00:00, 115.81it/s]\n",
      "[20 / 50]   Val: Loss = 0.07188, Accuracy = 94.60%: 100%|██████████| 13/13 [00:00<00:00, 90.74it/s]\n",
      "[21 / 50] Train: Loss = 0.03509, Accuracy = 95.13%: 100%|██████████| 572/572 [00:05<00:00, 113.99it/s]\n",
      "[21 / 50]   Val: Loss = 0.07282, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 93.03it/s]\n",
      "[22 / 50] Train: Loss = 0.03451, Accuracy = 95.08%: 100%|██████████| 572/572 [00:04<00:00, 116.16it/s]\n",
      "[22 / 50]   Val: Loss = 0.08510, Accuracy = 94.57%: 100%|██████████| 13/13 [00:00<00:00, 97.64it/s]\n",
      "[23 / 50] Train: Loss = 0.03411, Accuracy = 95.11%: 100%|██████████| 572/572 [00:04<00:00, 115.33it/s]\n",
      "[23 / 50]   Val: Loss = 0.08368, Accuracy = 94.67%: 100%|██████████| 13/13 [00:00<00:00, 96.23it/s]\n",
      "[24 / 50] Train: Loss = 0.03343, Accuracy = 95.15%: 100%|██████████| 572/572 [00:04<00:00, 115.71it/s]\n",
      "[24 / 50]   Val: Loss = 0.08088, Accuracy = 94.64%: 100%|██████████| 13/13 [00:00<00:00, 90.04it/s]\n",
      "[25 / 50] Train: Loss = 0.03342, Accuracy = 95.14%: 100%|██████████| 572/572 [00:04<00:00, 116.01it/s]\n",
      "[25 / 50]   Val: Loss = 0.08451, Accuracy = 94.60%: 100%|██████████| 13/13 [00:00<00:00, 93.42it/s]\n",
      "[26 / 50] Train: Loss = 0.03353, Accuracy = 95.13%: 100%|██████████| 572/572 [00:04<00:00, 115.85it/s]\n",
      "[26 / 50]   Val: Loss = 0.08786, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 91.49it/s]\n",
      "[27 / 50] Train: Loss = 0.03329, Accuracy = 95.18%: 100%|██████████| 572/572 [00:05<00:00, 114.19it/s]\n",
      "[27 / 50]   Val: Loss = 0.09230, Accuracy = 94.51%: 100%|██████████| 13/13 [00:00<00:00, 96.74it/s]\n",
      "[28 / 50] Train: Loss = 0.03285, Accuracy = 95.17%: 100%|██████████| 572/572 [00:04<00:00, 115.12it/s]\n",
      "[28 / 50]   Val: Loss = 0.09513, Accuracy = 94.54%: 100%|██████████| 13/13 [00:00<00:00, 96.08it/s]\n",
      "[29 / 50] Train: Loss = 0.03283, Accuracy = 95.18%: 100%|██████████| 572/572 [00:04<00:00, 115.05it/s]\n",
      "[29 / 50]   Val: Loss = 0.09755, Accuracy = 94.56%: 100%|██████████| 13/13 [00:00<00:00, 96.75it/s] \n",
      "[30 / 50] Train: Loss = 0.03289, Accuracy = 95.18%: 100%|██████████| 572/572 [00:04<00:00, 116.46it/s]\n",
      "[30 / 50]   Val: Loss = 0.10085, Accuracy = 94.36%: 100%|██████████| 13/13 [00:00<00:00, 89.64it/s]\n",
      "[31 / 50] Train: Loss = 0.03256, Accuracy = 95.18%: 100%|██████████| 572/572 [00:04<00:00, 115.68it/s]\n",
      "[31 / 50]   Val: Loss = 0.09944, Accuracy = 94.41%: 100%|██████████| 13/13 [00:00<00:00, 92.65it/s]\n",
      "[32 / 50] Train: Loss = 0.03221, Accuracy = 95.17%: 100%|██████████| 572/572 [00:05<00:00, 113.48it/s]\n",
      "[32 / 50]   Val: Loss = 0.09270, Accuracy = 94.49%: 100%|██████████| 13/13 [00:00<00:00, 89.59it/s]\n",
      "[33 / 50] Train: Loss = 0.03236, Accuracy = 95.14%: 100%|██████████| 572/572 [00:04<00:00, 116.83it/s]\n",
      "[33 / 50]   Val: Loss = 0.10456, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 95.04it/s]\n",
      "[34 / 50] Train: Loss = 0.03215, Accuracy = 95.16%: 100%|██████████| 572/572 [00:04<00:00, 115.28it/s]\n",
      "[34 / 50]   Val: Loss = 0.10417, Accuracy = 94.61%: 100%|██████████| 13/13 [00:00<00:00, 94.47it/s]\n",
      "[35 / 50] Train: Loss = 0.03234, Accuracy = 95.19%: 100%|██████████| 572/572 [00:04<00:00, 117.62it/s]\n",
      "[35 / 50]   Val: Loss = 0.09597, Accuracy = 94.75%: 100%|██████████| 13/13 [00:00<00:00, 91.92it/s]\n",
      "[36 / 50] Train: Loss = 0.03207, Accuracy = 95.20%: 100%|██████████| 572/572 [00:04<00:00, 115.58it/s]\n",
      "[36 / 50]   Val: Loss = 0.09490, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 88.06it/s]\n",
      "[37 / 50] Train: Loss = 0.03207, Accuracy = 95.18%: 100%|██████████| 572/572 [00:05<00:00, 114.34it/s]\n",
      "[37 / 50]   Val: Loss = 0.10283, Accuracy = 94.80%: 100%|██████████| 13/13 [00:00<00:00, 84.68it/s]\n",
      "[38 / 50] Train: Loss = 0.03195, Accuracy = 95.20%: 100%|██████████| 572/572 [00:04<00:00, 116.00it/s]\n",
      "[38 / 50]   Val: Loss = 0.10470, Accuracy = 94.80%: 100%|██████████| 13/13 [00:00<00:00, 95.68it/s]\n",
      "[39 / 50] Train: Loss = 0.03183, Accuracy = 95.23%: 100%|██████████| 572/572 [00:04<00:00, 115.96it/s]\n",
      "[39 / 50]   Val: Loss = 0.09936, Accuracy = 95.03%: 100%|██████████| 13/13 [00:00<00:00, 92.48it/s]\n",
      "[40 / 50] Train: Loss = 0.03160, Accuracy = 95.15%: 100%|██████████| 572/572 [00:04<00:00, 115.41it/s]\n",
      "[40 / 50]   Val: Loss = 0.08368, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 91.49it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.03188, Accuracy = 95.18%: 100%|██████████| 572/572 [00:05<00:00, 111.49it/s]\n",
      "[41 / 50]   Val: Loss = 0.09696, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 97.53it/s]\n",
      "[42 / 50] Train: Loss = 0.03195, Accuracy = 95.25%: 100%|██████████| 572/572 [00:04<00:00, 114.72it/s]\n",
      "[42 / 50]   Val: Loss = 0.10151, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 94.41it/s]\n",
      "[43 / 50] Train: Loss = 0.03154, Accuracy = 95.23%: 100%|██████████| 572/572 [00:05<00:00, 112.98it/s]\n",
      "[43 / 50]   Val: Loss = 0.10211, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 95.93it/s]\n",
      "[44 / 50] Train: Loss = 0.03184, Accuracy = 95.20%: 100%|██████████| 572/572 [00:04<00:00, 117.47it/s]\n",
      "[44 / 50]   Val: Loss = 0.09101, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 87.41it/s]\n",
      "[45 / 50] Train: Loss = 0.03147, Accuracy = 95.22%: 100%|██████████| 572/572 [00:05<00:00, 113.90it/s]\n",
      "[45 / 50]   Val: Loss = 0.10139, Accuracy = 94.93%: 100%|██████████| 13/13 [00:00<00:00, 94.01it/s]\n",
      "[46 / 50] Train: Loss = 0.03150, Accuracy = 95.23%: 100%|██████████| 572/572 [00:04<00:00, 115.82it/s]\n",
      "[46 / 50]   Val: Loss = 0.09952, Accuracy = 95.09%: 100%|██████████| 13/13 [00:00<00:00, 89.17it/s]\n",
      "[47 / 50] Train: Loss = 0.03139, Accuracy = 95.21%: 100%|██████████| 572/572 [00:04<00:00, 116.41it/s]\n",
      "[47 / 50]   Val: Loss = 0.10070, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 95.46it/s]\n",
      "[48 / 50] Train: Loss = 0.03156, Accuracy = 95.25%: 100%|██████████| 572/572 [00:04<00:00, 115.83it/s]\n",
      "[48 / 50]   Val: Loss = 0.09848, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 93.20it/s]\n",
      "[49 / 50] Train: Loss = 0.03159, Accuracy = 95.21%: 100%|██████████| 572/572 [00:04<00:00, 115.86it/s]\n",
      "[49 / 50]   Val: Loss = 0.10126, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 94.59it/s]\n",
      "[50 / 50] Train: Loss = 0.03131, Accuracy = 95.21%: 100%|██████████| 572/572 [00:04<00:00, 115.78it/s]\n",
      "[50 / 50]   Val: Loss = 0.11138, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 98.01it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitaly/PycharmProjects/dl_course_ai/venv/lib/python3.6/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self._tagset_size = tagset_size\n",
    "        self._lstm_hidden_dim = lstm_hidden_dim\n",
    "        self._lstm_layer_count = lstm_layers_count\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.emb(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.76886, Accuracy = 79.87%: 100%|██████████| 572/572 [00:03<00:00, 167.26it/s]\n",
      "[1 / 50]   Val: Loss = 0.36718, Accuracy = 89.01%: 100%|██████████| 13/13 [00:00<00:00, 143.17it/s]\n",
      "[2 / 50] Train: Loss = 0.30412, Accuracy = 90.81%: 100%|██████████| 572/572 [00:03<00:00, 175.54it/s]\n",
      "[2 / 50]   Val: Loss = 0.26935, Accuracy = 91.69%: 100%|██████████| 13/13 [00:00<00:00, 140.13it/s]\n",
      "[3 / 50] Train: Loss = 0.23423, Accuracy = 92.66%: 100%|██████████| 572/572 [00:03<00:00, 170.23it/s]\n",
      "[3 / 50]   Val: Loss = 0.22611, Accuracy = 92.88%: 100%|██████████| 13/13 [00:00<00:00, 144.51it/s]\n",
      "[4 / 50] Train: Loss = 0.19768, Accuracy = 93.71%: 100%|██████████| 572/572 [00:03<00:00, 173.52it/s]\n",
      "[4 / 50]   Val: Loss = 0.19787, Accuracy = 93.61%: 100%|██████████| 13/13 [00:00<00:00, 148.89it/s]\n",
      "[5 / 50] Train: Loss = 0.17554, Accuracy = 94.32%: 100%|██████████| 572/572 [00:03<00:00, 173.16it/s]\n",
      "[5 / 50]   Val: Loss = 0.18341, Accuracy = 93.90%: 100%|██████████| 13/13 [00:00<00:00, 134.20it/s]\n",
      "[6 / 50] Train: Loss = 0.16041, Accuracy = 94.73%: 100%|██████████| 572/572 [00:03<00:00, 170.84it/s]\n",
      "[6 / 50]   Val: Loss = 0.17167, Accuracy = 94.25%: 100%|██████████| 13/13 [00:00<00:00, 143.11it/s]\n",
      "[7 / 50] Train: Loss = 0.14954, Accuracy = 95.03%: 100%|██████████| 572/572 [00:03<00:00, 168.62it/s]\n",
      "[7 / 50]   Val: Loss = 0.16698, Accuracy = 94.42%: 100%|██████████| 13/13 [00:00<00:00, 142.05it/s]\n",
      "[8 / 50] Train: Loss = 0.14166, Accuracy = 95.26%: 100%|██████████| 572/572 [00:03<00:00, 169.23it/s]\n",
      "[8 / 50]   Val: Loss = 0.15905, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 138.47it/s]\n",
      "[9 / 50] Train: Loss = 0.13455, Accuracy = 95.46%: 100%|██████████| 572/572 [00:03<00:00, 171.85it/s]\n",
      "[9 / 50]   Val: Loss = 0.15648, Accuracy = 94.69%: 100%|██████████| 13/13 [00:00<00:00, 143.64it/s]\n",
      "[10 / 50] Train: Loss = 0.12934, Accuracy = 95.62%: 100%|██████████| 572/572 [00:03<00:00, 166.53it/s]\n",
      "[10 / 50]   Val: Loss = 0.15220, Accuracy = 94.86%: 100%|██████████| 13/13 [00:00<00:00, 117.25it/s]\n",
      "[11 / 50] Train: Loss = 0.12525, Accuracy = 95.77%: 100%|██████████| 572/572 [00:03<00:00, 169.53it/s]\n",
      "[11 / 50]   Val: Loss = 0.15028, Accuracy = 94.89%: 100%|██████████| 13/13 [00:00<00:00, 145.88it/s]\n",
      "[12 / 50] Train: Loss = 0.12110, Accuracy = 95.86%: 100%|██████████| 572/572 [00:03<00:00, 170.67it/s]\n",
      "[12 / 50]   Val: Loss = 0.14739, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 145.54it/s]\n",
      "[13 / 50] Train: Loss = 0.11779, Accuracy = 95.95%: 100%|██████████| 572/572 [00:03<00:00, 174.71it/s]\n",
      "[13 / 50]   Val: Loss = 0.14576, Accuracy = 95.03%: 100%|██████████| 13/13 [00:00<00:00, 142.06it/s]\n",
      "[14 / 50] Train: Loss = 0.11470, Accuracy = 96.05%: 100%|██████████| 572/572 [00:03<00:00, 166.23it/s]\n",
      "[14 / 50]   Val: Loss = 0.14391, Accuracy = 95.09%: 100%|██████████| 13/13 [00:00<00:00, 137.35it/s]\n",
      "[15 / 50] Train: Loss = 0.11298, Accuracy = 96.12%: 100%|██████████| 572/572 [00:03<00:00, 167.69it/s]\n",
      "[15 / 50]   Val: Loss = 0.14339, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 151.88it/s]\n",
      "[16 / 50] Train: Loss = 0.10992, Accuracy = 96.17%: 100%|██████████| 572/572 [00:03<00:00, 168.13it/s]\n",
      "[16 / 50]   Val: Loss = 0.14226, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 142.52it/s]\n",
      "[17 / 50] Train: Loss = 0.10803, Accuracy = 96.23%: 100%|██████████| 572/572 [00:03<00:00, 168.69it/s]\n",
      "[17 / 50]   Val: Loss = 0.14247, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 143.57it/s]\n",
      "[18 / 50] Train: Loss = 0.10646, Accuracy = 96.27%: 100%|██████████| 572/572 [00:03<00:00, 169.30it/s]\n",
      "[18 / 50]   Val: Loss = 0.13962, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 147.29it/s]\n",
      "[19 / 50] Train: Loss = 0.10470, Accuracy = 96.34%: 100%|██████████| 572/572 [00:03<00:00, 167.69it/s]\n",
      "[19 / 50]   Val: Loss = 0.14129, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 153.18it/s]\n",
      "[20 / 50] Train: Loss = 0.10317, Accuracy = 96.38%: 100%|██████████| 572/572 [00:03<00:00, 168.68it/s]\n",
      "[20 / 50]   Val: Loss = 0.13938, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 133.93it/s]\n",
      "[21 / 50] Train: Loss = 0.10169, Accuracy = 96.42%: 100%|██████████| 572/572 [00:03<00:00, 170.58it/s]\n",
      "[21 / 50]   Val: Loss = 0.13990, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 137.32it/s]\n",
      "[22 / 50] Train: Loss = 0.10022, Accuracy = 96.47%: 100%|██████████| 572/572 [00:03<00:00, 169.70it/s]\n",
      "[22 / 50]   Val: Loss = 0.13797, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 152.66it/s]\n",
      "[23 / 50] Train: Loss = 0.09874, Accuracy = 96.53%: 100%|██████████| 572/572 [00:03<00:00, 169.06it/s]\n",
      "[23 / 50]   Val: Loss = 0.13792, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 142.97it/s]\n",
      "[24 / 50] Train: Loss = 0.09782, Accuracy = 96.56%: 100%|██████████| 572/572 [00:03<00:00, 171.27it/s]\n",
      "[24 / 50]   Val: Loss = 0.13913, Accuracy = 95.19%: 100%|██████████| 13/13 [00:00<00:00, 136.12it/s]\n",
      "[25 / 50] Train: Loss = 0.09612, Accuracy = 96.59%: 100%|██████████| 572/572 [00:03<00:00, 171.78it/s]\n",
      "[25 / 50]   Val: Loss = 0.14072, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 147.41it/s]\n",
      "[26 / 50] Train: Loss = 0.09530, Accuracy = 96.61%: 100%|██████████| 572/572 [00:03<00:00, 171.67it/s]\n",
      "[26 / 50]   Val: Loss = 0.13822, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 148.32it/s]\n",
      "[27 / 50] Train: Loss = 0.09436, Accuracy = 96.66%: 100%|██████████| 572/572 [00:03<00:00, 171.67it/s]\n",
      "[27 / 50]   Val: Loss = 0.13883, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 149.55it/s]\n",
      "[28 / 50] Train: Loss = 0.09379, Accuracy = 96.68%: 100%|██████████| 572/572 [00:03<00:00, 174.17it/s]\n",
      "[28 / 50]   Val: Loss = 0.13946, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 154.08it/s]\n",
      "[29 / 50] Train: Loss = 0.09276, Accuracy = 96.71%: 100%|██████████| 572/572 [00:03<00:00, 170.80it/s]\n",
      "[29 / 50]   Val: Loss = 0.13699, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 153.42it/s]\n",
      "[30 / 50] Train: Loss = 0.09110, Accuracy = 96.76%: 100%|██████████| 572/572 [00:03<00:00, 172.98it/s]\n",
      "[30 / 50]   Val: Loss = 0.13956, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 146.24it/s]\n",
      "[31 / 50] Train: Loss = 0.09084, Accuracy = 96.78%: 100%|██████████| 572/572 [00:03<00:00, 176.75it/s]\n",
      "[31 / 50]   Val: Loss = 0.13847, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 139.24it/s]\n",
      "[32 / 50] Train: Loss = 0.08999, Accuracy = 96.80%: 100%|██████████| 572/572 [00:03<00:00, 171.54it/s]\n",
      "[32 / 50]   Val: Loss = 0.13725, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 146.33it/s]\n",
      "[33 / 50] Train: Loss = 0.08884, Accuracy = 96.84%: 100%|██████████| 572/572 [00:03<00:00, 171.22it/s]\n",
      "[33 / 50]   Val: Loss = 0.13850, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 156.64it/s]\n",
      "[34 / 50] Train: Loss = 0.08787, Accuracy = 96.87%: 100%|██████████| 572/572 [00:03<00:00, 174.73it/s]\n",
      "[34 / 50]   Val: Loss = 0.13761, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 145.04it/s]\n",
      "[35 / 50] Train: Loss = 0.08753, Accuracy = 96.88%: 100%|██████████| 572/572 [00:03<00:00, 172.66it/s]\n",
      "[35 / 50]   Val: Loss = 0.14086, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 145.50it/s]\n",
      "[36 / 50] Train: Loss = 0.08663, Accuracy = 96.91%: 100%|██████████| 572/572 [00:03<00:00, 173.01it/s]\n",
      "[36 / 50]   Val: Loss = 0.13820, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 148.57it/s]\n",
      "[37 / 50] Train: Loss = 0.08561, Accuracy = 96.94%: 100%|██████████| 572/572 [00:03<00:00, 176.99it/s]\n",
      "[37 / 50]   Val: Loss = 0.13734, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 145.92it/s]\n",
      "[38 / 50] Train: Loss = 0.08483, Accuracy = 96.96%: 100%|██████████| 572/572 [00:03<00:00, 175.30it/s]\n",
      "[38 / 50]   Val: Loss = 0.14016, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 149.27it/s]\n",
      "[39 / 50] Train: Loss = 0.08389, Accuracy = 96.99%: 100%|██████████| 572/572 [00:03<00:00, 171.86it/s]\n",
      "[39 / 50]   Val: Loss = 0.13993, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 148.91it/s]\n",
      "[40 / 50] Train: Loss = 0.08413, Accuracy = 96.99%: 100%|██████████| 572/572 [00:03<00:00, 172.34it/s]\n",
      "[40 / 50]   Val: Loss = 0.13939, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 141.46it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.08299, Accuracy = 97.04%: 100%|██████████| 572/572 [00:03<00:00, 171.76it/s]\n",
      "[41 / 50]   Val: Loss = 0.13931, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 130.86it/s]\n",
      "[42 / 50] Train: Loss = 0.08242, Accuracy = 97.06%: 100%|██████████| 572/572 [00:03<00:00, 173.74it/s]\n",
      "[42 / 50]   Val: Loss = 0.14021, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 146.79it/s]\n",
      "[43 / 50] Train: Loss = 0.08169, Accuracy = 97.08%: 100%|██████████| 572/572 [00:03<00:00, 167.88it/s]\n",
      "[43 / 50]   Val: Loss = 0.14156, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 150.37it/s]\n",
      "[44 / 50] Train: Loss = 0.08141, Accuracy = 97.09%: 100%|██████████| 572/572 [00:03<00:00, 171.46it/s]\n",
      "[44 / 50]   Val: Loss = 0.13958, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 143.24it/s]\n",
      "[45 / 50] Train: Loss = 0.08092, Accuracy = 97.10%: 100%|██████████| 572/572 [00:03<00:00, 177.45it/s]\n",
      "[45 / 50]   Val: Loss = 0.14019, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 139.46it/s]\n",
      "[46 / 50] Train: Loss = 0.08009, Accuracy = 97.11%: 100%|██████████| 572/572 [00:03<00:00, 170.18it/s]\n",
      "[46 / 50]   Val: Loss = 0.14201, Accuracy = 95.19%: 100%|██████████| 13/13 [00:00<00:00, 149.27it/s]\n",
      "[47 / 50] Train: Loss = 0.08005, Accuracy = 97.13%: 100%|██████████| 572/572 [00:03<00:00, 170.28it/s]\n",
      "[47 / 50]   Val: Loss = 0.14054, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 153.43it/s]\n",
      "[48 / 50] Train: Loss = 0.07919, Accuracy = 97.17%: 100%|██████████| 572/572 [00:03<00:00, 175.51it/s]\n",
      "[48 / 50]   Val: Loss = 0.14189, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 144.19it/s]\n",
      "[49 / 50] Train: Loss = 0.07865, Accuracy = 97.18%: 100%|██████████| 572/572 [00:03<00:00, 175.82it/s]\n",
      "[49 / 50]   Val: Loss = 0.14140, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 156.01it/s]\n",
      "[50 / 50] Train: Loss = 0.07826, Accuracy = 97.19%: 100%|██████████| 572/572 [00:03<00:00, 173.46it/s]\n",
      "[50 / 50]   Val: Loss = 0.14173, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 150.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_sum_count = 0\n",
    "cur_correct_count = 0\n",
    "\n",
    "for X_batch, y_batch in iterate_batches((X_test, y_test), 100):\n",
    "\n",
    "    X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "    X_batch, y_batch = X_batch.to(torch.device('cuda:0')), y_batch.to(torch.device('cuda:0'))\n",
    "\n",
    "    logits = model(X_batch)\n",
    "\n",
    "    # seq_len, batch_size, target_size\n",
    "    # для каждого элемента из последовательности длинной seq_len есть вектор размера target_size. \n",
    "    # таких последовательностей в количестве batch_size\n",
    "\n",
    "    mask = y_batch != 0\n",
    "    logit_mask = logits.argmax(dim=2) * mask\n",
    "    y_batch_mask = y_batch * mask\n",
    "    cur_sum_count += mask.sum()\n",
    "    cur_correct_count += ((logit_mask == y_batch_mask).sum() - (~mask).sum()).float()\n",
    "\n",
    "acc = cur_correct_count / cur_sum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9517, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
